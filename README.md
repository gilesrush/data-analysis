

中日仏教文献セマンティック分析ツールセット - 取扱説明書
1. プロジェクト概要
このツールセットは、中国語と日本語のテキストコーパス（特に仏教文献）に対して、詳細なセマンティック分析（意味分析）を行うための一連のPythonスクリプトです。テキストのクリーニング前処理から、Word2Vecを用いた単語埋め込みモデルのトレーニング、さらに多角的な定量的分析と可視化まで、一貫した研究フローを提供します。

2. 主な機能
テキストの前処理: テキストをクリーニングし、CJK統合漢字（漢字）と日本語のかなのみを保持することで、後続の分析のために純粋なコーパスを準備します。

単語埋め込みモデルのトレーニング: 中国語と日本語のコーパスそれぞれに対して独立したWord2Vecモデルをトレーニングし、単語をベクトル形式に変換してその意味情報を捉えます。

言語横断的な意味比較:

自動近傍単語分析: 中国語と日本語のモデルにおいて、同じ単語（共有語彙）のセマンティックな近傍単語（意味が近い単語）がどのように異なるかを比較します。

特定分野（仏教）のアライメント分析: プロクラステス分析による直交変換を用いて、中日2つの単語ベクトル空間を整列（アライメント）させ、特に仏教の核心的な専門用語が両言語でどのような意味的差異を持つかを探ります。

テキスト間の類似度分析: TF-IDFとコサイン類似度を用いて、コーパス内の全ドキュメント間の類似度を算出し、相互テキスト性（intertextuality）の近接行列を生成します。

意味空間の可視化:

t-SNEによる次元削減技術を利用し、高次元のドキュメント関係（WMD距離に基づく）を2次元平面にマッピングし、異なる文献が意味空間上でどのように分布し、クラスタリングされているかを直感的に表示します。

Louvainコミュニティ検出アルゴリズムを用いて、文献における意味的なクラスタ（コミュニティ）を自動的に検出します。

3. 環境準備 (Prerequisites)
いずれかのスクリプトを実行する前に、Python環境をセットアップし、必要なサードパーティライブラリをすべてインストールする必要があります。

3.1. Pythonのバージョン

Python 3.8 以降のバージョンを推奨します。

3.2. 必要なライブラリ (Extensions)
pip を使用して、すべての依存関係を一度にインストールできます。ターミナルまたはコマンドラインツールを開き、以下のコマンドを実行してください。

Bash

pip install pandas numpy scipy gensim jieba sudachipy scikit-learn matplotlib networkx python-louvain tqdm sudachidict_core
pandas: データ処理およびCSVファイルの読み書きに使用します。

numpy: 効率的な数値計算と行列演算に使用します。

scipy: 科学技術計算に使用します。このプロジェクトでは主に直交プロクラステス分析で使用します。

gensim: Word2Vecモデルのトレーニングに使用する中心的なライブラリの一つです。

jieba: 中国語の形態素解析（分かち書き）ライブラリです。

sudachipy & sudachidict_core: 日本語の形態素解析ライブラリとそのコア辞書です。

scikit-learn: TF-IDFベクトル化、コサイン類似度計算、t-SNE次元削減に使用します。

matplotlib: データの可視化、グラフの生成に使用します。

networkx & python-louvain: ネットワークグラフの構築とコミュニティ検出に使用します。

tqdm: コマンドラインに美しいプログレスバーを表示します。

4. ファイル構造 (File Structure)
スクリプトが正しく動作するように、以下の構造でファイルを整理してください。

your_project_folder/
│
├── corpus/                # .txt形式のテキストコーパスを格納する
│   ├── 1001_some_text_ch.txt
│   ├── 1002_another_text_ch.txt
│   ├── 2025_text_jp.txt
│   └── 2026_another_text_jp.txt
│
├── analyze.py             # スクリプト
├── Cleaner (1).py         # スクリプト
├── compare.py             # スクリプト
├── result1.py             # スクリプト
├── Word2Vec.py            # スクリプト
├── wmd_tsne_louvain.py    # スクリプト
│
├── custom.dic             # (任意) compare.pyで使用するカスタム辞書
├── skip_phrases.txt       # (任意) analyze.pyで使用する除外フレーズ
├── stopwords.txt          # (任意) analyze.pyで使用するストップワード
└── user_dict.txt          # (任意) analyze.pyで使用するJiebaのユーザー辞書
重要：corpusフォルダについて

コーパスの配置: すべての中国語および日本語の.txtドキュメントをこのフォルダに配置してください。

言語判定: Word2Vec.py および analyze.py スクリプトは、ファイル名に基づいて言語を判定します。ルールは通常以下の通りです。

ファイル名が2000または2025より小さい数字で始まる場合（例: 1001.txt, 1998_doc.txt）、中国語と見なされます。

ファイル名が2000または2025以上の数字で始まる場合（例: 2025_doc.txt, 2030.txt）、日本語と見なされます。

この命名規則に必ず従ってください。従わない場合、モデルのトレーニングや分析でエラーが発生します。

5. 使用手順と説明 (Execution Order & Instructions)
以下の推奨手順に従ってスクリプトを実行してください。

ステップ1: テキストのクリーニング (Preprocessing)
スクリプト: Cleaner (1).py

役割: corpus フォルダ内のすべての .txt ファイルを走査し、漢字と日本語のかな以外のすべての文字（数字、英字、句読点など）を削除し、クリーニングされた内容で元のファイルを上書きします。

コマンド:

Bash

python "Cleaner (1).py"
生成物: corpus フォルダ内のファイル内容が浄化されます。

ステップ2: 単語埋め込みモデルのトレーニング (Word2Vec Model Training)
スクリプト: Word2Vec.py

役割: corpus 内のすべてのテキストを読み込み、ファイル名に基づいて言語を判定した後、中国語と日本語のコーパスそれぞれに対して形態素解析を行い、2つの独立したWord2Vecモデルをトレーニングします。

コマンド:

Bash

python Word2Vec.py
生成物:

model_chinese.model: トレーニング済みの中国語単語埋め込みモデル。

model_japanese.model: トレーニング済みの日本語単語埋め込みモデル。

ステップ3: 分析パスの選択
モデルのトレーニングが完了したら、研究ニーズに応じて異なる分析パスを選択できます。パスAとパスB/Cは互いに独立しており、どちらからでも実行できます。

パスA: 言語横断的な意味比較 (Cross-lingual Semantic Comparison)
このパスは、ステップ2で生成された.modelファイルに依存します。

A1. バッチ自動比較 (compare.py)

スクリプト: compare.py

役割: 中国語と日本語のモデルにおける共有語彙を見つけ出し、ベクトル空間を整列させます。その後、共有語彙がそれぞれの言語モデルでどの単語と最も意味的に類似しているかを自動的に計算し、テンプレート化された説明を含むCSVファイルを生成します。

準備: 単語の定義を検索するために、word,definitionの2列からなるcustom.dicというCSVファイルが必要です。このファイルが存在しない場合、定義は「無定義」と表示されます。

コマンド:

Bash

python compare.py
生成物: semantic_neighbors_offline.csv - 中心となる単語、それぞれの近傍単語、類似度、そして初期的な差異説明を含む表。

A2. 対話的な仏教用語の探索 (result1.py)

スクリプト: result1.py

役割: 豊富な仏教用語リストを「アンカーワード」として特別に用い、2つのモデル空間を整列させます。その後、クエリ単語（デフォルトでは空）を指定すると、その単語が中国語のネイティブ空間、日本語のネイティブ空間、そして整列後の空間でそれぞれどのような類似単語を持つかを表示します。

使用方法:

result1.py ファイルを開きます。

query_word = "空" という行を見つけます。

その中の"空"を、検索したい任意の共有語彙に変更します。

コマンド:

Bash

python result1.py
生成物: コマンドラインのターミナルに直接比較結果が出力されます。

パスB: テキスト間の類似度分析 (Inter-textual Similarity)
スクリプト: analyze.py

役割: corpus 内のすべてのドキュメント間で、TF-IDFとコサイン類似度を計算し、テキスト間の「相互テキスト性」や内容の類似性を測定します。

準備 (任意):

stopwords.txt: 中国語と日本語のストップワードを定義します。

skip_phrases.txt: テキストから全体として削除する必要がある定型句を定義します。

user_dict.txt: Jieba形態素解析器にカスタム辞書を提供します。

コマンド:

Bash

python analyze.py
生成物: intertextual_proximity_matrix.csv - 行と列がファイル名で、値がそれらの間の類似度スコアである正方行列。

パスC: 意味空間の可視化 (Semantic Space Visualization)
スクリプト: wmd_tsne_louvain.py

重要：前提条件: このスクリプトはwmd_results.csvというファイルに依存しますが、提供されたスクリプトの中にこのファイルを生成するものはありません。このファイルには、ドキュメントペア間の「Word Mover's Distance (WMD)」が含まれている必要があります。WMDを計算するためには、別途スクリプトを作成する必要があります。そのスクリプトでは、ステップ2で生成されたmodel_chinese.modelとmodel_japanese.modelをロードし、corpus内のすべてのドキュメントペア間のWMD値を計算する必要があります。

wmd_results.csvを生成済みと仮定した場合、そのフォーマットは以下のようになります:

Code snippet

Document_A,Document_B,Distance
1001.txt,1002.txt,0.85
1001.txt,2025.txt,1.23
...
役割: WMD距離行列を読み込み、t-SNEを用いて次元削減と可視化を行い、Louvainアルゴリズムでコミュニティ検出を行い、最終的に2枚のグラフを生成します。

コマンド:

Bash

python wmd_tsne_louvain.py
生成物:

semantic_space_map.png: t-SNEによる次元削減プロット。中国語と日本語の文献が異なる色で区別されています。

semantic_space_modularity.png: t-SNEプロットを基に、Louvainアルゴリズムが検出した文献コミュニティを異なる色で表示したものです。

