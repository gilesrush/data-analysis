# -*- coding: utf-8 -*-
import os
import re
import jieba
import pandas as pd
from sudachipy import tokenizer
from sudachipy import dictionary
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# --- è¨­å®š ---
CORPUS_PATH = "corpus"
SKIP_PHRASE_FILE = "skip_phrases.txt"
STOPWORDS_PATH = "stopwords.txt"
USER_DICT_PATH = "user_dict.txt"
NGRAM_RANGE = (1, 2)
MIN_DF = 2
SUDACHI_LIMIT = 49149

# --- å®šå‹å¥èª­ã¿è¾¼ã¿ ---
def load_skip_phrases(filepath):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰å®šå‹å¥ã‚’èª­ã¿è¾¼ã‚“ã§ã‚»ãƒƒãƒˆã¨ã—ã¦è¿”ã™ã€‚"""
    if not os.path.exists(filepath):
        return set()
    with open(filepath, 'r', encoding='utf-8') as f:
        return set(line.strip() for line in f if line.strip())

SKIP_PHRASES = load_skip_phrases(SKIP_PHRASE_FILE)

def remove_skip_phrases(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å®šå‹å¥ã‚’å‰Šé™¤ã™ã‚‹ã€‚"""
    for phrase in SKIP_PHRASES:
        text = text.replace(phrase, "")
    return text

# --- ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿ ---
def load_stopwords(filepath):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã‚“ã§ã‚»ãƒƒãƒˆã¨ã—ã¦è¿”ã™ã€‚"""
    if not os.path.exists(filepath):
        print(f"è­¦å‘Š: ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ« '{filepath}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return set()
    with open(filepath, 'r', encoding='utf-8') as f:
        return set(line.strip() for line in f)

stopwords = load_stopwords(STOPWORDS_PATH)

# --- SudachiPy åˆæœŸåŒ– ---
tokenizer_obj = dictionary.Dictionary().create()
mode = tokenizer.Tokenizer.SplitMode.C

# --- Jieba ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸èª­ã¿è¾¼ã¿ ---
try:
    jieba.load_userdict(USER_DICT_PATH)
except FileNotFoundError:
    print(f"è­¦å‘Š: ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ '{USER_DICT_PATH}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

# --- è¨€èªåˆ¤å®š ---
def detect_language(filename):
    """ãƒ•ã‚¡ã‚¤ãƒ«åã«åŸºã¥ã„ã¦è¨€èªã‚’åˆ¤å®šã™ã‚‹ï¼ˆ'chinese' ã¾ãŸã¯ 'japanese'ï¼‰ã€‚"""
    base = filename.split('.')[0]
    match = re.match(r'^(\d+)', base)
    # ãƒ•ã‚¡ã‚¤ãƒ«åãŒæ•°å­—ã§å§‹ã¾ã‚Šã€ãã®æ•°å­—ãŒ2000ä»¥ä¸Šãªã‚‰æ—¥æœ¬èªã¨åˆ¤å®š
    if match:
        year = int(match.group(1))
        return 'japanese' if year >= 2000 else 'chinese'
    # ãã‚Œä»¥å¤–ã¯ä¸­å›½èªã¨åˆ¤å®š
    return 'chinese'

# --- SudachiPy åˆ†å‰²ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º ---
def tokenize_japanese_safely(text):
    """SudachiPyã®æ–‡å­—æ•°åˆ¶é™ã‚’å›é¿ã™ã‚‹ãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ã—ã¦ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã™ã‚‹ã€‚"""
    byte_text = text.encode("utf-8")
    if len(byte_text) <= SUDACHI_LIMIT:
        return [m.surface() for m in tokenizer_obj.tokenize(text, mode) if m.surface() not in stopwords]

    tokens = []
    print("âš ï¸ SudachiPyã®æ–‡å­—æ•°åˆ¶é™ã‚’è¶…éã—ãŸãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ã—ã¦å‡¦ç†ã—ã¦ã„ã¾ã™...")
    start = 0
    while start < len(byte_text):
        end = min(start + SUDACHI_LIMIT - 1000, len(byte_text))
        chunk = byte_text[start:end]
        # UTF-8ã®æ–‡å­—å¢ƒç•Œã§åˆ†å‰²ã‚’ç¢ºå®Ÿã«ã™ã‚‹
        while True:
            try:
                chunk_text = chunk.decode("utf-8")
                break
            except UnicodeDecodeError:
                end -= 1
                chunk = byte_text[start:end]

        tokens.extend(m.surface() for m in tokenizer_obj.tokenize(chunk_text, mode) if m.surface() not in stopwords)
        start = end
    return tokens

# --- å‰å‡¦ç†ãƒ»åˆ†è© ---
def preprocess_and_tokenize(text, language):
    """ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ï¼ˆè¨˜å·å‰Šé™¤ãªã©ï¼‰ã¨åˆ†è©å‡¦ç†ã‚’è¡Œã†ã€‚"""
    text = re.sub(r"[0-9a-zA-Zï½-ï½šï¼¡-ï¼ºã€‚ã€ï¼ãƒ»ã€Œã€ï¼ˆï¼‰ã€ã€ã€ã€‘ï¼»ï¼½ã€ˆã€‰ã€Šã€‹â€œâ€â€˜â€™ï¼ï¼Ÿï¼Œã€ï¼šï¼›ï¼ˆï¼‰â€¦\s]", "", text)
    text = remove_skip_phrases(text)
    if language == 'chinese':
        return [w for w in jieba.cut(text) if w and w not in stopwords]
    elif language == 'japanese':
        return tokenize_japanese_safely(text)
    return []

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main():
    """
    ã‚³ãƒ¼ãƒ‘ã‚¹å†…ã®æ–‡æ›¸ã‚’å‡¦ç†ã—ã€TF-IDFã«åŸºã¥ã„ãŸã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¡Œåˆ—ã‚’è¨ˆç®—ã—ã¦CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã™ã‚‹ã€‚
    """
    print("ğŸ§˜ æ–‡æ›¸ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
    documents = []
    raw_docs = []
    filenames = []

    try:
        corpus_files = sorted([f for f in os.listdir(CORPUS_PATH) if f.endswith(".txt")])
        if not corpus_files:
            print(f"ğŸ›‘ ã‚¨ãƒ©ãƒ¼: '{CORPUS_PATH}' ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«.txtãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return
    except FileNotFoundError:
        print(f"ğŸ›‘ ã‚¨ãƒ©ãƒ¼: ã‚³ãƒ¼ãƒ‘ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{CORPUS_PATH}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    for filename in corpus_files:
        path = os.path.join(CORPUS_PATH, filename)
        with open(path, 'r', encoding='utf-8') as f:
            raw_text = f.read()

        lang = detect_language(filename)
        tokens = preprocess_and_tokenize(raw_text, lang)

        if len(tokens) < 5:
            print(f"â© {filename} ã¯å†…å®¹ãŒå°‘ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
            continue

        # åˆ†è©çµæœã‚’ã‚¹ãƒšãƒ¼ã‚¹ã§é€£çµã—ã¦æ–‡æ›¸ãƒªã‚¹ãƒˆã«è¿½åŠ 
        raw_docs.append(" ".join(tokens))
        filenames.append(filename)
        print(f"ğŸ“„ {filename} ã‚’å‡¦ç†ã—ã¾ã—ãŸï¼ˆè¨€èª: {lang}, ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(tokens)}ï¼‰")

    if not raw_docs:
        print("ğŸ›‘ å‡¦ç†ã§ãã‚‹æœ‰åŠ¹ãªæ–‡æ›¸ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    print("\nğŸ“Š TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®è¨ˆç®—ã‚’å®Ÿè¡Œä¸­...")
    # TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ã®åˆæœŸåŒ–
    vectorizer = TfidfVectorizer(ngram_range=NGRAM_RANGE, min_df=MIN_DF)
    # TF-IDFè¡Œåˆ—ã®è¨ˆç®—
    tfidf_matrix = vectorizer.fit_transform(raw_docs)
    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¡Œåˆ—ã®è¨ˆç®—
    similarity_matrix = cosine_similarity(tfidf_matrix)

    # çµæœã‚’DataFrameã«å¤‰æ›
    df_sim = pd.DataFrame(similarity_matrix, index=filenames, columns=filenames)
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    output_filename = "intertextual_proximity_matrix.csv"
    df_sim.to_csv(output_filename, encoding='utf-8-sig')
    print(f"âœ” TF-IDFé¡ä¼¼åº¦è¡Œåˆ—ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã—ã¾ã—ãŸ: {output_filename}")

    # --- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰æ©Ÿèƒ½ã¯å‰Šé™¤æ¸ˆã¿ ---

    print("\nâœ¨ å‡¦ç†å®Œäº†ã€‚")


if __name__ == "__main__":
    main()