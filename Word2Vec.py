# -*- coding: utf-8 -*-
import os
import re
import jieba
from gensim.models import Word2Vec
from sudachipy import dictionary, tokenizer
from tqdm import tqdm  # âœ… å‹•çš„é€²æ—è¡¨ç¤ºç”¨

# === è¨­å®š ===
CORPUS_PATH = "corpus"
MODEL_PATH_CH = "model_chinese.model"
MODEL_PATH_JP = "model_japanese.model"
MIN_COUNT = 3
VECTOR_SIZE = 100
WINDOW = 5
EPOCHS = 10
SUDACHI_LIMIT = 49149  # âœ… SudachiPy ã®æœ€å¤§å…¥åŠ›åˆ¶é™ï¼ˆãƒã‚¤ãƒˆï¼‰

# === SudachiPy åˆæœŸåŒ– ===
sudachi = dictionary.Dictionary().create()
mode = tokenizer.Tokenizer.SplitMode.C

# === ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆä¸­æ—¥æ¼¢å­—ã¨ä»®åã®ã¿æ®‹ã™ï¼‰===
def clean_text(text):
    text = re.sub(r"[^\u4e00-\u9fa5\u3040-\u30ff\u3400-\u9fff]", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

# === åˆ†ã‹ã¡æ›¸ãé–¢æ•°ï¼ˆé•·æ–‡å¯¾å¿œï¼‰===
def tokenize_text(text, lang):
    text = clean_text(text)
    if lang == "chinese":
        return list(jieba.cut(text))
    else:
        tokens = []
        text_bytes = text.encode("utf-8")
        start = 0
        while start < len(text_bytes):
            end = min(start + SUDACHI_LIMIT, len(text_bytes))
            # UTF-8ã®æ–‡å­—ã®é€”ä¸­ã§åˆ‡ã‚‰ãªã„ã‚ˆã†ã«èª¿æ•´
            while end < len(text_bytes) and (text_bytes[end] & 0b11000000) == 0b10000000:
                end -= 1
            chunk = text_bytes[start:end].decode("utf-8", errors="ignore")
            for m in sudachi.tokenize(chunk, mode):
                word = m.dictionary_form()
                if word and word != " ":
                    tokens.append(word)
            start = end
        return tokens

# === ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰è¨€èªåˆ¤å®š ===
def detect_language(filename):
    base = os.path.splitext(filename)[0]
    match = re.match(r"(\d+)([A-Za-z]*)", base)
    if not match:
        return "chinese"
    num, suffix = match.groups()
    return "japanese" if int(num) >= 2025 and not suffix else "chinese"

# === æ–‡æ›¸å‡¦ç†ã¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ ===
sentences_ch = []
sentences_jp = []

print("ğŸ“‚ corpus ãƒ•ã‚©ãƒ«ãƒ€ã®ã‚¹ã‚­ãƒ£ãƒ³ã‚’é–‹å§‹...")

file_list = [f for f in os.listdir(CORPUS_PATH) if f.endswith(".txt")]

for fname in tqdm(file_list, desc="æ–‡æ›¸å‡¦ç†ä¸­", ncols=100):
    tqdm.write(f"ğŸ“„ å‡¦ç†ä¸­ï¼š{fname}")
    path = os.path.join(CORPUS_PATH, fname)
    with open(path, encoding="utf-8") as f:
        text = f.read()
    lang = detect_language(fname)
    tokens = tokenize_text(text, lang)
    if len(tokens) >= MIN_COUNT:
        if lang == "chinese":
            sentences_ch.append(tokens)
        else:
            sentences_jp.append(tokens)

print("\nğŸ“Š æ–‡çŒ®çµ±è¨ˆï¼š")
print(f" - ä¸­å›½èªæ–‡çŒ®æ•°ï¼š{len(sentences_ch)} ä»¶")
print(f" - æ—¥æœ¬èªæ–‡çŒ®æ•°ï¼š{len(sentences_jp)} ä»¶")

# === Word2Vec ãƒ¢ãƒ‡ãƒ«è¨“ç·´ ===
if sentences_ch:
    print("ğŸ§  ä¸­å›½èª Word2Vec ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")
    model_ch = Word2Vec(
        sentences=sentences_ch,
        vector_size=VECTOR_SIZE,
        window=WINDOW,
        min_count=MIN_COUNT,
        workers=4,
        epochs=EPOCHS
    )
    model_ch.save(MODEL_PATH_CH)
    print(f"âœ… ä¸­å›½èªãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼š{MODEL_PATH_CH}")
else:
    print("âš  ä¸­å›½èªæ–‡çŒ®ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

if sentences_jp:
    print("ğŸ§  æ—¥æœ¬èª Word2Vec ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")
    model_jp = Word2Vec(
        sentences=sentences_jp,
        vector_size=VECTOR_SIZE,
        window=WINDOW,
        min_count=MIN_COUNT,
        workers=4,
        epochs=EPOCHS
    )
    model_jp.save(MODEL_PATH_JP)
    print(f"âœ… æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼š{MODEL_PATH_JP}")
else:
    print("âš  æ—¥æœ¬èªæ–‡çŒ®ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
